{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names, load_dataset\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import gensim.parsing.preprocessing as pre\n",
    "from gensim.models import KeyedVectors\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from wefe.query import Query\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from wefe.metrics.WEAT import WEAT\n",
    "from wefe.utils import plot_queries_results, run_queries\n",
    "\n",
    "TRAIN_EMBED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikitext-103-v1', 'wikitext-2-v1', 'wikitext-103-raw-v1', 'wikitext-2-raw-v1']\n"
     ]
    }
   ],
   "source": [
    "configs = get_dataset_config_names(\"wikitext\")\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][\"text\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = gensim.downloader.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(wv):\n",
    "    # Given examples\n",
    "    output = []\n",
    "    x0 = wv[\"piano\"]\n",
    "    output.append(x0)\n",
    "    x1 = wv[\"cherry\"] - wv[\"michigan\"] + wv[\"georgia\"]\n",
    "    output.append(x1)\n",
    "    # Classic\n",
    "    x2 = wv[\"king\"] - wv[\"man\"] + wv[\"woman\"]\n",
    "    output.append(x2)\n",
    "    x3 = wv[\"zoo\"] + wv[\"pet\"]\n",
    "    output.append(x3)\n",
    "    x4 = wv[\"game\"] - wv[\"computer\"]\n",
    "    output.append(x4)\n",
    "    x5 = wv[\"orchestra\"] - wv[\"flute\"] + wv[\"saxophone\"]\n",
    "    output.append(x5)\n",
    "    x6 = wv[\"dinosaur\"] + wv[\"fly\"]\n",
    "    output.append(x6)\n",
    "    x7 = wv[\"book\"] + wv[\"digital\"]\n",
    "    output.append(x7)\n",
    "    output = [wv.most_similar(x) for x in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('digital', 0.7597694396972656),\n",
       " ('book', 0.7047815918922424),\n",
       " ('books', 0.6316168308258057),\n",
       " ('eBook', 0.6188921332359314),\n",
       " ('hardcover_paperback', 0.6001059412956238),\n",
       " ('ebook', 0.5957868695259094),\n",
       " ('downloadable_audiobook', 0.5945138931274414),\n",
       " ('ebooks', 0.5858449339866638),\n",
       " ('hardbound_edition', 0.5780873894691467),\n",
       " ('eBooks', 0.5696583390235901)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_embeddings(g_news)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_EMBED:\n",
    "    preprocess = [pre.strip_multiple_whitespaces, pre.lower_to_unicode, pre.strip_punctuation, pre.strip_tags]\n",
    "    sentences = [pre.preprocess_string(x[\"text\"], preprocess) for x in data[\"train\"] if len(x[\"text\"]) > 3]\n",
    "    cbow = gensim.models.Word2Vec(sentences, vector_size=64, workers=8, epochs=10)\n",
    "    cbow.wv.save(\"cbow.wordvectors\")\n",
    "else:\n",
    "    cbow = KeyedVectors.load(\"cbow.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_EMBED:\n",
    "    skip = gensim.models.Word2Vec(sentences, vector_size=64, workers=8, epochs=10, sg=1)\n",
    "    skip.wv.save(\"skip.wordvectors\")\n",
    "else:\n",
    "    skip = KeyedVectors.load(\"skip.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_EMBED:\n",
    "    skip = skip.wv\n",
    "    cbow = cbow.wv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>method</th><th>query</th><th>nearest</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;cbow&quot;</td><td>&quot;piano&quot;</td><td>&quot;piano,violin,c…</td></tr><tr><td>&quot;cbow&quot;</td><td>&quot;cherry - michi…</td><td>&quot;ginger,cherry,…</td></tr><tr><td>&quot;cbow&quot;</td><td>&quot;king - man + w…</td><td>&quot;queen,king,emp…</td></tr><tr><td>&quot;cbow&quot;</td><td>&quot;zoo + pet&quot;</td><td>&quot;pet,zoo,goldfi…</td></tr><tr><td>&quot;cbow&quot;</td><td>&quot;game - compute…</td><td>&quot;season,game,ma…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌────────┬─────────────────────────────┬───────────────────────────────────┐\n",
       "│ method ┆ query                       ┆ nearest                           │\n",
       "│ ---    ┆ ---                         ┆ ---                               │\n",
       "│ str    ┆ str                         ┆ str                               │\n",
       "╞════════╪═════════════════════════════╪═══════════════════════════════════╡\n",
       "│ cbow   ┆ piano                       ┆ piano,violin,cello,harpsichord,m… │\n",
       "│ cbow   ┆ cherry - michigan + georgia ┆ ginger,cherry,howlin,poppin,meat… │\n",
       "│ cbow   ┆ king - man + woman          ┆ queen,king,empress,monarch,dowag… │\n",
       "│ cbow   ┆ zoo + pet                   ┆ pet,zoo,goldfish,aquarium,cat,sa… │\n",
       "│ cbow   ┆ game - computer             ┆ season,game,matchup,postseason,i… │\n",
       "└────────┴─────────────────────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"piano\", \"cherry - michigan + georgia\", \"king - man + woman\", \"zoo + pet\", \"game - computer\", \"orchestra - flute + saxophone\", \n",
    "           \"dinosaur + fly\", \"book + digital\"]\n",
    "records = []\n",
    "# emb = compare_embeddings(skip.wv)\n",
    "methods = {\"cbow\": cbow, \"skip\": skip, \n",
    "           \"twitter\": twitter, \"g_news\": g_news}\n",
    "\n",
    "for method, wv in methods.items():\n",
    "    emb = compare_embeddings(wv)\n",
    "    emb = [\",\".join([x[0] for x in list]) for list in emb]\n",
    "    for k, v in zip(queries, emb):\n",
    "        record = {\"method\": method, \"query\": k, \"nearest\": v}\n",
    "        records.append(record)\n",
    "\n",
    "df = pl.from_records(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data1.tex\", \"w\") as f:\n",
    "    f.write(r\"\\begin{lstlisting}\")\n",
    "    f.write(\"\\n\")\n",
    "    for group in df.group_by(pl.col(\"query\")):\n",
    "        query = group[0]\n",
    "        f.write(f\"{query}\\n\")\n",
    "        for row in group[1].select([\"method\", \"nearest\"]).rows():\n",
    "            if len(row[1]) > 60:\n",
    "                first = \" \".join(row[1].split(\",\")[0:5])\n",
    "                second = \" \".join(row[1].split(\",\")[5:])\n",
    "                f.write(f\"{row[0]}: {first}\\n\\t\\t{second}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{row[0]}: {row[1]}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.write(r\"\\end{lstlisting}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wefe example query\n",
    "def wefe_example(model):\n",
    "    target_sets = [['she', 'woman', 'girl'], ['he', 'man', 'boy']]\n",
    "    target_sets_names = ['Female Terms', 'Male Terms']\n",
    "    attribute_sets = [['poetry','dance','literature'], ['math', 'physics', 'chemistry']]\n",
    "    attribute_sets_names = ['Arts', 'Science']\n",
    "    query = Query(target_sets, attribute_sets, target_sets_names,\n",
    "                attribute_sets_names)\n",
    "    # instance a WEAT metric\n",
    "    weat = WEAT()\n",
    "    result = weat.run_query(query, model, calculate_p_value=True)\n",
    "    return result\n",
    "\n",
    "gender_bias = {}\n",
    "for (name, wv) in methods.items():\n",
    "    gender_bias[name] = wefe_example(WordEmbeddingModel(wv, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cbow': {'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       "  'result': 0.46166253089904785,\n",
       "  'weat': 0.46166253089904785,\n",
       "  'effect_size': 1.357256198626603,\n",
       "  'p_value': 0.038834951456310676},\n",
       " 'skip': {'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       "  'result': 0.41074570020039886,\n",
       "  'weat': 0.41074570020039886,\n",
       "  'effect_size': 1.3333383112146575,\n",
       "  'p_value': 0.047156726768377254},\n",
       " 'twitter': {'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       "  'result': 0.07661843299865723,\n",
       "  'weat': 0.07661843299865723,\n",
       "  'effect_size': 0.806086739803075,\n",
       "  'p_value': 0.15811373092926492},\n",
       " 'g_news': {'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       "  'result': 0.2539586052298546,\n",
       "  'weat': 0.2539586052298546,\n",
       "  'effect_size': 1.8524392657902091,\n",
       "  'p_value': 0.018030513176144243}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cbow': {'query_name': 'Asian and Western wrt Music and Sports',\n",
       "  'result': 0.009961791336536518,\n",
       "  'weat': 0.009961791336536518,\n",
       "  'effect_size': 0.02770080402656437,\n",
       "  'p_value': 0.4618585298196949},\n",
       " 'skip': {'query_name': 'Asian and Western wrt Music and Sports',\n",
       "  'result': -0.06884972999493277,\n",
       "  'weat': -0.06884972999493277,\n",
       "  'effect_size': -0.2100393069555281,\n",
       "  'p_value': 0.5963938973647711},\n",
       " 'twitter': {'query_name': 'Asian and Western wrt Music and Sports',\n",
       "  'result': 0.42101111014684045,\n",
       "  'weat': 0.42101111014684045,\n",
       "  'effect_size': 1.4366601251904048,\n",
       "  'p_value': 0.05131761442441054},\n",
       " 'g_news': {'query_name': 'Asian and Western wrt Music and Sports',\n",
       "  'result': 0.23539705574512482,\n",
       "  'weat': 0.23539705574512482,\n",
       "  'effect_size': 1.2973573123875393,\n",
       "  'p_value': 0.08183079056865465}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wefe_example(model):\n",
    "    target_sets = [['asian', 'chinese', 'japanese'], ['white', 'european', 'american']]\n",
    "    target_sets_names = [\"Asian\", \"Western\"]\n",
    "    attribute_sets = [[\"piano\", \"violin\", \"prodigy\"], [\"sport\", \"sports\", \"athlete\"]]\n",
    "    attribute_sets_names = [\"Music\", \"Sports\"]\n",
    "    query = Query(target_sets, attribute_sets, target_sets_names,\n",
    "                attribute_sets_names)\n",
    "    weat = WEAT()\n",
    "    result = weat.run_query(query, model, calculate_p_value=True)\n",
    "    return result\n",
    "\n",
    "run_queries(WEAT, )\n",
    "culture_bias = {}\n",
    "for (name, wv) in methods.items():\n",
    "    culture_bias[name] = wefe_example(WordEmbeddingModel(wv, name))\n",
    "culture_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
